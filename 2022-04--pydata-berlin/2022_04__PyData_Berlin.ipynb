{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f45ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.8.0\n",
    "# !pip install spektral==1.0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32544b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import spektral as spktrl\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "\n",
    "from spektral.datasets import Citation, TUDataset\n",
    "from spektral.data import SingleLoader, DisjointLoader\n",
    "from spektral.transforms import LayerPreprocess\n",
    "from spektral.data import Dataset, Graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Using Spektral {spktrl.__version__}')\n",
    "print(f'Using TensorFlow {tf.__version__}')\n",
    "print('Physical GPUs:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edddd1ac",
   "metadata": {},
   "source": [
    "# Practical graph neural networks in Python with TensorFlow and Spektral\n",
    "\n",
    "*PyData Berlin, 2022.04.13*\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "\n",
    "Graph neural networks (GNNs) have become one of the hottest research topics in recent years. Their popularity is reinforced by hugely successful industry applications in social networks, biology, chemistry, neuroscience and many other areas. One of the main challenges faced by data scientists and researchers who want to apply graph networks in their work is that they require different data structures and a slightly different training approach than traditional deep learning models. During the workshop we’ll demonstrate how to implement graph neural networks, how to prepare your data and – finally – how to train a GNN model for node-level and graph-level tasks using Spektral and TensorFlow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecd400",
   "metadata": {},
   "source": [
    "## 1. Node classification with functional API\n",
    "\n",
    "We'll perform node classification using [CORA](https://relational.fit.cvut.cz/dataset/CORA) citation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff02a2",
   "metadata": {},
   "source": [
    "### 1.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc400285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = Citation(\"cora\", normalize_x=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a3073",
   "metadata": {},
   "source": [
    "### 1.2 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand the adjacency matrix\n",
    "dataset[0].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a549b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand labels\n",
    "dataset[0].y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929e4c2",
   "metadata": {},
   "source": [
    "#### Exercise 1.2.1\n",
    "\n",
    "Display the label of node 77. \n",
    "\n",
    "\n",
    "What is the label of this node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7284bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let't understand features\n",
    "dataset[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdcd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of non-zero featueres over nodes\n",
    "plt.hist((dataset[0].x > 0).sum(axis=1), alpha=.7, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the masks \n",
    "\n",
    "# Training, val, test \n",
    "dataset.mask_tr, dataset.mask_va, dataset.mask_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae7db5",
   "metadata": {},
   "source": [
    "#### Exercise 1.2.2\n",
    "\n",
    "Compute the number of training, validation and test examples. \n",
    "\n",
    "What are these numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "print(f'Number of training examples: {...}')\n",
    "print(f'Number of validation examples: {dataset.mask_va.sum()}')\n",
    "print(f'Number of test examples: {...}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946711df",
   "metadata": {},
   "source": [
    "### 1.3 Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_tr = SingleLoader(dataset)\n",
    "loader_va = SingleLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962029a",
   "metadata": {},
   "source": [
    "### 1.4 Build and compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434cfe42",
   "metadata": {},
   "source": [
    "#### 1.4.1 Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35571d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "in_x = keras.Input(shape=(dataset[0].x.shape[1],))\n",
    "in_a = keras.Input(shape=(dataset[0].a.shape[0],), sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout on features (but not adjacency matrix)\n",
    "dropout_1 = keras.layers.Dropout(.1)(in_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a91e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add GAT layer\n",
    "gat_layer_1 = spktrl.layers.GATConv(\n",
    "    channels=16,\n",
    "    attn_heads=8,\n",
    "    concat_heads=True,\n",
    "    dropout_rate=.05,\n",
    "    activation='selu',\n",
    "    kernel_initializer='lecun_normal'\n",
    ")([dropout_1, in_a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout\n",
    "dropout_2 = keras.layers.Dropout(.1)(gat_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945040e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final GAT layer\n",
    "gat_out = spktrl.layers.GATConv(\n",
    "    channels=dataset[0].n_labels,\n",
    "    attn_heads=8,\n",
    "    concat_heads=False,\n",
    "    dropout_rate=.05,\n",
    "    activation='softmax'\n",
    ")([dropout_2, in_a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enclose the layers in the model\n",
    "model = keras.Model(inputs=[in_x, in_a], outputs=gat_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968496c1",
   "metadata": {},
   "source": [
    "#### 1.4.2 Setup and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd73740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some params\n",
    "LR = 5e-3 # 5e-3  # Learning rate\n",
    "EPOCHS = 10000  # Number of training epochs\n",
    "PATIENCE = 30  # Patience for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(lr=LR)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.CategoricalCrossentropy(reduction='sum'),\n",
    "    weighted_metrics=['acc'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d66d0",
   "metadata": {},
   "source": [
    "#### 1.4.3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369279a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=PATIENCE, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=PATIENCE//2, min_lr=5e-6, factor=.9)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    loader_tr.load(),\n",
    "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
    "    validation_data=loader_va.load(),\n",
    "    validation_steps=loader_va.steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train', lw=2)\n",
    "plt.plot(history.history['val_loss'], label='Val', lw=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22c511",
   "metadata": {},
   "source": [
    "### 1.5 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"Evaluating model.\")\n",
    "loader_te = SingleLoader(dataset)\n",
    "eval_results = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
    "print(\"Done.\\n\" \"Test loss: {}\\n\" \"Test accuracy: {}\".format(*eval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90d3b6",
   "metadata": {},
   "source": [
    "#### Exercise 1.4.1\n",
    "\n",
    "Add one more hidden GAT layer, build, compile, train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ec539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "in_x = keras.Input(shape=(dataset[0].x.shape[1],))\n",
    "in_a = keras.Input(shape=(dataset[0].a.shape[0],), sparse=True)\n",
    "\n",
    "# Add dropout on features (but not adjacency matrix)\n",
    "dropout_1 = keras.layers.Dropout(.1)(in_x)\n",
    "\n",
    "# Add GAT layer\n",
    "gat_layer_1 = spktrl.layers.GATConv(\n",
    "    channels=16,\n",
    "    attn_heads=8,\n",
    "    concat_heads=True,\n",
    "    dropout_rate=.05,\n",
    "    activation='selu',\n",
    "    kernel_initializer='lecun_normal'\n",
    ")([dropout_1, in_a])\n",
    "\n",
    "# Add dropout\n",
    "dropout_2 = keras.layers.Dropout(.1)(gat_layer_1)\n",
    "\n",
    "\n",
    "\n",
    "######## YOUR CODE STARTS HERE ########\n",
    "\n",
    "# Add another GAT layer\n",
    "gat_layer_2 = ...\n",
    "\n",
    "# Add another dropout layer\n",
    "dropout_3 = ...\n",
    "\n",
    "######## YOUR CODE ENDS HERE ########\n",
    "\n",
    "\n",
    "# Final GAT layer\n",
    "gat_out = spktrl.layers.GATConv(\n",
    "    channels=dataset[0].n_labels,\n",
    "    attn_heads=8,\n",
    "    concat_heads=False,\n",
    "    dropout_rate=.05,\n",
    "    activation='softmax'\n",
    ")([dropout_3, in_a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enclose the layers in the model\n",
    "model = keras.Model(inputs=[in_x, in_a], outputs=gat_out)\n",
    "\n",
    "# Set some params\n",
    "LR = 5e-3 # 5e-3  # Learning rate\n",
    "EPOCHS = 10000  # Number of training epochs\n",
    "PATIENCE = 30  # Patience for early stopping\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(lr=LR)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.CategoricalCrossentropy(reduction='sum'),\n",
    "    weighted_metrics=['acc'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc49a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=PATIENCE, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=PATIENCE//2, min_lr=5e-6, factor=.9)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "######## YOUR CODE STARTS HERE ########\n",
    "history = model.fit(\n",
    "    loader_tr.load(),\n",
    "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
    "    validation_data=loader_va.load(),\n",
    "    validation_steps=...,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=...\n",
    ")\n",
    "######## YOUR CODE ENDS HERE ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train', lw=2)\n",
    "plt.plot(history.history['val_loss'], label='Val', lw=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47217a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"Evaluating model.\")\n",
    "loader_te = SingleLoader(dataset)\n",
    "eval_results = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
    "print(\"Done.\\n\" \"Test loss: {}\\n\" \"Test accuracy: {}\".format(*eval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce3473",
   "metadata": {},
   "source": [
    "## 2. Graph classification with model sub-classing API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e09359",
   "metadata": {},
   "source": [
    "We'll use **Proteins** dataset, a part of [TU Datasets](https://chrsmrrs.github.io/datasets/).\n",
    "\n",
    "Proteins dataset is stored in a **disjoint** format.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/disjoint.png\" width=400>\n",
    "\n",
    "\n",
    "We'll need not only adjacency matrix and feature matrix, but also index matrix to identify which nodes belong to which batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3ec90",
   "metadata": {},
   "source": [
    "### 2.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(\"PROTEINS\", clean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7076559",
   "metadata": {},
   "source": [
    "#### Exercise 2.1.1\n",
    "\n",
    "Check how many nodes are in the 8th graph of **Proteins** dataset.\n",
    "\n",
    "How many are there in 172nd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10022eed",
   "metadata": {},
   "source": [
    "### 2.2 Split + dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae672425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "idxs = np.random.permutation(len(dataset))  # Random split\n",
    "split = int(0.9 * len(dataset))\n",
    "idx_tr, idx_te = np.split(idxs, [split])\n",
    "\n",
    "# Get train and test datsets\n",
    "dataset_tr, dataset_te = dataset[idx_tr], dataset[idx_te]\n",
    "\n",
    "# Get loaders \n",
    "loader_tr = DisjointLoader(dataset_tr, batch_size=32, epochs=10)\n",
    "loader_te = DisjointLoader(dataset_te, batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e79139",
   "metadata": {},
   "source": [
    "### 2.3 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c709a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(keras.models.Model):\n",
    "    \n",
    "    def __init__(self, channels, n_layers, dropout_rate=.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = spktrl.layers.GCNConv(channels)\n",
    "        self.convs = []\n",
    "        \n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                spktrl.layers.GCNConv(channels)\n",
    "            )\n",
    "        self.pool = spktrl.layers.GlobalAvgPool()\n",
    "        self.dense1 = keras.layers.Dense(channels, activation='relu')\n",
    "        self.dropout = keras.layers.Dropout(dropout_rate)\n",
    "        self.dense2 = keras.layers.Dense(dataset.n_labels, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80546ca3",
   "metadata": {},
   "source": [
    "### 2.3 Compile, train & evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9ec3f",
   "metadata": {},
   "source": [
    "#### 2.3.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18eceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some params\n",
    "LR = 5e-3 # 5e-3  # Learning rate\n",
    "EPOCHS = 10  # Number of training epochs\n",
    "PATIENCE = 30  # Patience for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774074c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = GCN(\n",
    "    channels=16,\n",
    "    dropout_rate=.1,\n",
    "    n_layers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = keras.optimizers.RMSprop(LR)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.CategoricalCrossentropy(reduction='sum'),\n",
    "    weighted_metrics=['acc'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48612ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    loader_tr.load(),\n",
    "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
    "    validation_data=loader_te.load(),\n",
    "    validation_steps=loader_te.steps_per_epoch,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfdacfd",
   "metadata": {},
   "source": [
    "#### Exercise 2.3.1\n",
    "\n",
    "Train a GCN with:\n",
    "\n",
    "* 32 channels \n",
    "* 6 layers\n",
    "* Adam optimizer (use the same learning rate, `LR`)\n",
    "\n",
    "Are the results better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f265e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loaders \n",
    "loader_tr = DisjointLoader(dataset_tr, batch_size=32, epochs=10)\n",
    "loader_te = DisjointLoader(dataset_te, batch_size=32, epochs=1)\n",
    "\n",
    "######## YOUR CODE STARTS HERE ########\n",
    "model = ...\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = ...\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.CategoricalCrossentropy(reduction='sum'),\n",
    "    weighted_metrics=['acc'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    loader_tr.load(),\n",
    "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
    "    validation_data=loader_te.load(),\n",
    "    validation_steps=loader_te.steps_per_epoch,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be30e85a",
   "metadata": {},
   "source": [
    "## 3. Building a custom dataset\n",
    "\n",
    "To build your own dataset, you should store your data in a specific location. \n",
    "\n",
    "Locally it's: `~/.spektral/datasets/[ClassName]`\n",
    "\n",
    "You can overwrite it by defining the `path` property of a `Dataset` class. \n",
    "\n",
    "\n",
    "\n",
    "Path on **Colab**: `/usr/local/lib/python3.7/dist-packages/spectral/datasets`\n",
    "\n",
    "\n",
    "___________________________\n",
    "\n",
    "<img src=\"img/tensorcell.png\" width=150>\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we're going to look at a dataset class that we used in one of our experiments at [TensorCell](https://www.tensorcell.com/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bccdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorcellDataset(Dataset):\n",
    "    \n",
    "    \"\"\"A Tensorcell dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_variant, allow_self_loops=True, circular_mapping=False, add_constant_feature=False, add_one_hot_index=False, **kwargs):\n",
    "        \"\"\"\n",
    "        :param dataset_variant: A dataset to pick. Currently takes: `ochota_100k`, `centrum_100k`, `mokotow_100k`\n",
    "        :type dataset_variant: str\n",
    "        :param circular_mapping: If node values should be mapped to a unit circle\n",
    "        :type circular_dataset: bool\n",
    "\n",
    "        ...\n",
    "        :return: None\n",
    "        :rtype: None\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset_variant = dataset_variant\n",
    "        self.allow_self_loops = allow_self_loops\n",
    "        self.circular_mapping = circular_mapping\n",
    "        self.add_constant_feature = add_constant_feature\n",
    "        self.add_one_hot_index = add_one_hot_index\n",
    "        \n",
    "        # Construct filenames\n",
    "        dataset_info = dataset_variant.split('_')\n",
    "        district = dataset_info[0]\n",
    "        n_rows = dataset_info[1]\n",
    "        \n",
    "        self.filename_A = f'{district}_A.txt'\n",
    "        self.filename_Xy = f'{district}_X_{n_rows}.txt'\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def read(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        :return: output\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        \n",
    "        # We must return a list of Graph objects\n",
    "        output = []\n",
    "        \n",
    "        # Read files\n",
    "        adjacency_matrix = np.loadtxt(os.path.join(self.path, self.filename_A))\n",
    "        features = np.loadtxt(os.path.join(self.path, self.filename_Xy), delimiter=',')\n",
    "\n",
    "        # Add/remove self loops in the adjacency matrix\n",
    "        if self.allow_self_loops:\n",
    "            np.fill_diagonal(adjacency_matrix, 1)\n",
    "        else:\n",
    "            np.fill_diagonal(adjacency_matrix, 0)\n",
    "\n",
    "        \n",
    "        # Construct graph objects\n",
    "        for row in range(features.shape[0]):\n",
    "\n",
    "            # If `circular_mapping` -> map to a circular representation\n",
    "            if self.circular_mapping:\n",
    "                x = self.get_circular_components(features[row, :-1]).T\n",
    "            else:\n",
    "                x = features[row, :-1][:, np.newaxis]\n",
    "\n",
    "            # Add constant feature 1\n",
    "            if self.add_constant_feature:\n",
    "                x = np.hstack([x, np.ones(x.shape[0])[:, np.newaxis]])\n",
    "\n",
    "            # Add one-hot encoded node label\n",
    "            if self.add_one_hot_index:\n",
    "\n",
    "                x_plus_oh = []\n",
    "\n",
    "                for i, d in enumerate(x):\n",
    "                    one_hot_index = np.zeros(x.shape[0])\n",
    "                    one_hot_index[i] = 1\n",
    "                    x_plus_oh.append(np.hstack([d, one_hot_index]))\n",
    "\n",
    "                x = np.array(x_plus_oh)\n",
    "\n",
    "            # Construct a graph \n",
    "            output.append(\n",
    "                Graph(\n",
    "                    x=x, \n",
    "                    a=adjacency_matrix, \n",
    "                    y=features[row, -1])\n",
    "            )\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorcellDataset('ochota_100k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-spektral-minimal]",
   "language": "python",
   "name": "conda-env-tf-spektral-minimal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
